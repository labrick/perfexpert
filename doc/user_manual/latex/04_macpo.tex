\chapter{Using MACPO}
MACPO is an acronym for Memory Access Characterization for Performance Optimization. It is a tool that has been built to assist performance tuning of single- and multi-threaded C, C++ or Fortran applications. More specifically, MACPO is designed to provide insight into an application's memory usage patterns.

\section{A Quick Demonstration}
To demonstrate the functioning of MACPO, let's consider an example program. This program uses Pthreads to calculate the value of Pi using the Monte-Carlo method.

The following code shows the function that is executed by each thread:

\begin{verbatim}
void* thread_func (void* arg) {
  int idx, i, repeat;
  float x, y, z;
  thread_info_t* thread_info = (thread_info_t*) arg;

  for (repeat = 0; repeat < REPEAT_COUNT; repeat++) {
    for (i = 0; i < ITERATIONS; i++) {
      idx = i + thread_info->tid;
      x = random_numbers[idx % RANDOM_BUFFER_SIZE];
      y = random_numbers[(1 + idx) % RANDOM_BUFFER_SIZE];
      z = x * x + y * y;
      if (z <= 1) counts[thread_info->tid]++;
    }
  }
  pthread_exit(0);
}
\end{verbatim}

To compile the application using MACPO, indicating that we are interested in understanding the performance metrics associated with the \texttt{thread\_func} function, run the following commands:
\begin{verbatim}
$ macpo.sh --macpo:instrument=thread_func -c monte-carlo.cc
$ macpo.sh --macpo:instrument=thread_func monte-carlo.o -o monte-carlo -lpthread -lrt
\end{verbatim}

Runtime logs (macpo.out) are produced when the application is run:
\begin{verbatim}
$ ./monte-carlo
\end{verbatim}

To print performance metrics, use the \texttt{macpo-analyze} program.
\begin{verbatim}
$ macpo-analyze macpo.out
\end{verbatim}

This will produce an output that is similar to the one shown below:
\begin{verbatim}
Var "counts", seen 1668 times, estimated to cost 147.12 cycles on every access.
Stride of 0 cache lines was observed 1585 times (100.00%).

Level 1 data cache conflicts = 78.22% [################################        ]
Level 2 data cache conflicts = 63.37% [##########################              ]
NUMA data conflicts = 43.56%          [##################                      ]

Level 1 data cache reuse factor = 97.0% [####################################### ]
Level 2 data cache reuse factor = 3.0%  [##                                      ]
Level 3 data cache reuse factor = 0.0%  [                                        ]
\end{verbatim}

The output shows the estimated cost of memory accesses (147.12 cycles) to the variable \texttt{counts} in terms of cycles per access\footnote{A large number of cycles indicates poor memory performance. Causes of large values may be explained by the following metrics.}. The output also shows stride values (0 cache lines) observed in the accesses to the variable\footnote{A stride of 0 cache lines indicates that subsequent references are in the same cache line; this indicates good memory performance and is good for vectorization. A stride of $n$ indicates that subsequent references are $n$ cache lines apart; the larger $n$, the worse is the memory performance.}. As per the cache conflicts shown in the output, accesses to the variable \texttt{counts} suffer from both L1 and L2 cache conflicts (also known as cache thrashing). Using this knowledge, we can pad the \texttt{counts} array (\textit{i.e.}, add dummy bytes to the array) so that each thread running on a core and, thus, sharing an L1 cache as well as the shared L2 cache, which may be shared by other cores, accesses a different cache line. This optimization reduces the running time of the \texttt{thread\_func} routine from 9.14s to 3.17s.

The other information provided by MACPO is discussed below in Section \ref{sec:macpo-metrics}.

\section{When to Use MACPO}

MACPO may be useful to you in any of the following situations:

\begin{itemize}
	\item Perfexpert reports that L2 or L3 data access LCPI is high.
	\item Your program uses a lot of memory or it is memory-bound.
	\item CPU profiling does not show any interesting results.
\end{itemize}

If all (or most) of your application's memory accesses are irregular, you may be able to infer the optimizations applicable to your program. However, for such programs, MACPO metrics may not be able to assist you directly.

\section{MACPO Metrics}
\label{sec:macpo-metrics}

Performance information shown by MACPO can be grouped into two parts. The first part shows information that is applicable to the entire function being profiled. The second part shows information that is specific to the important variables in the function.

\subsection{Function-wide Performance Metrics}
Currently, function-wide information only shows the number of streams that were seen while compiling the function. A stream is a variable or data structure that may be accessed repeatedly in a uniform manner.

\subsection{Variable-specific Performance Metrics}
For each variable that is encountered a significant number of times, MACPO shows the following performance metrics:

\begin{description}
	\item[Estimated average cycles per access]\hfill \\
	MACPO collects metrics that allow it to calculate the approximate reuse distance of variables. The reuse distance, in turn, helps to estimate the specific level of the cache that the request would have originated from. This makes it possible to estimate the number of cycles spent in each memory access. This cost in terms of memory access is grouped by the variable name and averaged to show in this metric.

	\item[Dominant stride values and their percentages]\hfill \\
	A stride is the constant difference in bytes between the last memory access and the most recent memory access to a variable. MACPO computes the access strides in units of cache line size. This provides an indication of how well a code can be vectorized (stride of 0, \textit{i.e.}, sequential access, is best) and how one might optimize the code for better performance.

	\item[Cache conflicts (thrashing) for L1 and L2 data caches]\hfill \\
	Cache conflicts arise when multiple processors, each with at least one private cache, repeatedly claim exclusive access to a portion of memory. This metric shows the percentage of requests to each level of the cache that conflicted with another access to the same variable.

	\item[NUMA conflicts]\hfill \\
	Most modern processors exhibit non-uniform memory access costs. The cost of memory access depends on whether the processor that hosts the memory controller managing the memory address is the same as the processor accessing the memory address. This metric displays the percentage of observed memory accesses that conflicted with another memory access to the same variable at the NUMA level.

	\item[Reuse factors for L1, L2 and L3 data caches]\hfill \\
	From the observed reuse distance and a probabilistic model of set-associative caches, MACPO estimates whether a given memory access would be served out of L1 or would overflow the size of the cache, resulting in the memory access being served out of a higher (L2 or possibly L3) level of cache. This analysis permits MACPO to calculate the multicore reuse factor, which is a count of the number of times a given cache line is reused in a specific level of the cache.
\end{description}

\section{Improving Application Performance by Interpreting MACPO Metrics}

This section explains how to translate the MACPO metrics into source code changes to improve the performance of your application.

\begin{description}
	\item[Estimated average cycles per access]\hfill \\
	The cycles per access metric provides an overview of the performance of memory accesses to a specific variable. It makes it possible to identify whether a particular variable is suffering from memory performance bottleneck problems.

	\item[Dominant stride values and their percentages]\hfill \\
	Programs that have unit strides or small regular stride values generally execute faster than programs that have long or irregular access strides. There are several factors giving better memory access performance. Since data is typically fetched from memory as cache lines, unit strides increase reuse. Hardware prefetchers can recognize small regular patterns in data accesses and bring data into caches before it is referenced, thus reducing data access penalty. Virtual address to physical address translation can also be serviced more efficiently (using TLBs) when the code exhibits unit strides.

	\item[Cache conflicts (thrashing) for L1 and L2 data caches]\hfill \\
	Cache conflicts indicate thrashing between cache lines. By padding the data structures with additional (dummy) bytes, each thread can be made to access a different cache line, effectively removing cache conflicts.

	\item[NUMA conflicts]\hfill \\
	Most operating systems implement a first-touch policy by which the memory controller associated with the processor that first accesses the memory address ``owns'' the memory address. Memory accesses for the same address by a different processor result in NUMA conflicts.  As a result, NUMA conflicts typically arise when one thread initializes a portion of memory that is then accessed by different threads. To avoid NUMA conflicts, have each processor initialize its own memory.

	\item[Reuse factors for L1, L2 and L3 data caches]\hfill \\
	A low reuse factor indicates that a line is frequently evicted from the cache. The reuse factor can be improved by reducing the reuse distance of memory accesses.
\end{description}

\section{Command line options accepted by MACPO}

The various analyses in MACPO and the corresponding options are shown below:

\begin{tabular}[h]{|c|c|}\hline
\textbf{Analysis} & \textbf{MACPO option} \\\hline
Memory analysis & \texttt{--macpo:instrument} \\\hline
Alignment checking & \texttt{--macpo:check-alignment} \\\hline
Measure loop trip counts & \texttt{--macpo:record-tripcount} \\\hline
Count branch outcomes & \texttt{--macpo:record-branchpath} \\\hline
Measure gather/scatter accesses from vectorization & \texttt{--macpo:vector-strides} \\\hline
\end{tabular}

Each of these options accepts a program location (i.e. either a function or a loop specification). Function names can be used as identifiers for instrumentation (e.g. \texttt{--macpo:check-alignment=foo}). To tell MACPO to instrument a loop, pass either the name of the function that contains the loop of interest or the name of the file that contains the loop and the line number at which the loop starts separated by the \texttt{\#} character. For instance, to instrument the loop in \texttt{bar.c} that starts at line \texttt{273} for measuring trip counts, use the option: \texttt{--macpo:record-tripcount=bar.c\#273}. You may also use the option \texttt{--macpo:record-tripcount=foo\#273} to achieve the same effect.

MACPO also accepts other flags that influence it's operation. The following table shows the valid options.
\begin{tabular}[h]{|c|c|}\hline
\textbf{MACPO option} & \textbf{Purpose} \\\hline
\texttt{--macpo:backup-filename} & File to backup original source code to, before transforming it.\\\hline
\texttt{--macpo:no-compile} & Generate the transformed source code in \texttt{rose\_*} file(s) but don't compile it.\\\hline
\texttt{--macpo:enable-sampling} & Enable sampling mode when collecting measurements from instrumentation.\\\hline
\texttt{--macpo:disable-sampling} & Disable sampling mode when collecting measurements from instrumentation.\\\hline
\texttt{--macpo:profile-analysis} & Collect and display basic profiling information for MACPO's static analyses.\\\hline
\end{tabular}

